"""
advanced_main.py - Point d'entr√©e principal pour le syst√®me d'optimisation avanc√©
Orchestre tous les modules pour g√©n√©rer des emplois du temps de haute qualit√©
"""
import logging
import json
import time
from datetime import datetime
from typing import Dict, List, Optional

from optimizer_advanced import AdvancedScheduleOptimizer
from smart_scheduler import SmartScheduler
from conflict_resolver import ConflictResolver, IssueType, SeverityLevel

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class AdvancedSchedulingSystem:
    """Syst√®me complet d'optimisation d'emplois du temps"""
    
    def __init__(self, db_config=None):
        """Initialise le syst√®me complet"""
        logger.info("=== INITIALISATION DU SYST√àME AVANC√â ===")
        
        # Initialiser tous les modules
        self.optimizer = AdvancedScheduleOptimizer(db_config)
        self.smart_scheduler = SmartScheduler()
        self.conflict_resolver = ConflictResolver()
        
        # Configuration du syst√®me
        self.config = {
            "max_iterations": 3,
            "quality_threshold": 90,
            "time_limit_per_iteration": 300,  # 5 minutes
            "enable_auto_fixes": True,
            "target_metrics": {
                "zero_gaps": True,
                "min_blocks_percentage": 80,
                "max_teacher_amplitude": 5,
                "quality_score_target": 90
            }
        }
        
        logger.info("‚úì Syst√®me initialis√© avec tous les modules")
    
    def generate_optimal_schedule(self) -> Dict:
        """
        G√©n√®re un emploi du temps optimal avec it√©rations d'am√©lioration
        
        Returns:
            Dict: Solution finale avec analyse compl√®te
        """
        logger.info("=== D√âBUT DE LA G√âN√âRATION OPTIMIS√âE ===")
        start_time = time.time()
        
        best_solution = None
        best_score = 0
        iteration_history = []
        
        for iteration in range(1, self.config["max_iterations"] + 1):
            logger.info(f"\n--- IT√âRATION {iteration}/{self.config['max_iterations']} ---")
            
            # 1. G√©n√©ration de la solution
            solution = self._run_optimization_iteration(iteration)
            
            if solution is None:
                logger.error(f"√âchec de l'it√©ration {iteration}")
                continue
            
            # 2. Analyse de qualit√©
            quality_analysis = self.conflict_resolver.analyze_schedule_quality(solution)
            
            # 3. Enregistrer les r√©sultats
            iteration_result = {
                "iteration": iteration,
                "quality_score": quality_analysis["global_score"],
                "issues_count": len(quality_analysis["issues"]),
                "stats": solution["stats"],
                "timestamp": datetime.now().isoformat()
            }
            iteration_history.append(iteration_result)
            
            # 4. V√©rifier si c'est la meilleure solution
            if quality_analysis["global_score"] > best_score:
                best_solution = solution
                best_solution["quality_analysis"] = quality_analysis
                best_score = quality_analysis["global_score"]
                
                logger.info(f"‚úì Nouvelle meilleure solution: {best_score}/100")
            
            # 5. V√©rifier si on a atteint le seuil de qualit√©
            if best_score >= self.config["quality_threshold"]:
                logger.info(f"üéØ Seuil de qualit√© atteint ({best_score}%) - Arr√™t anticip√©")
                break
            
            # 6. Appliquer des corrections automatiques si n√©cessaire
            if iteration < self.config["max_iterations"]:
                self._apply_iteration_improvements(quality_analysis)
        
        total_time = time.time() - start_time
        
        # Compilation du r√©sultat final
        final_result = self._compile_final_result(
            best_solution, 
            iteration_history, 
            total_time
        )
        
        logger.info(f"=== G√âN√âRATION TERMIN√âE EN {total_time:.1f}s ===")
        logger.info(f"Score final: {final_result['final_score']}/100")
        logger.info(f"Objectifs atteints: {final_result['objectives_met']}/{len(self.config['target_metrics'])}")
        
        return final_result
    
    def _run_optimization_iteration(self, iteration: int) -> Optional[Dict]:
        """Ex√©cute une it√©ration d'optimisation"""
        logger.info(f"Lancement de l'optimisation - It√©ration {iteration}")
        
        try:
            # Ajuster les param√®tres selon l'it√©ration
            time_limit = self.config["time_limit_per_iteration"]
            if iteration > 1:
                time_limit *= 1.5  # Plus de temps pour les it√©rations suivantes
            
            # Lancer l'optimisation
            solution = self.optimizer.run_optimization()
            
            if solution:
                logger.info(f"‚úì Solution g√©n√©r√©e avec score: {solution['stats']['quality_score']}/100")
                return solution
            else:
                logger.warning("Aucune solution trouv√©e pour cette it√©ration")
                return None
                
        except Exception as e:
            logger.error(f"Erreur lors de l'it√©ration {iteration}: {e}")
            return None
    
    def _apply_iteration_improvements(self, quality_analysis: Dict):
        """Applique des am√©liorations bas√©es sur l'analyse de qualit√©"""
        logger.info("Application des am√©liorations pour la prochaine it√©ration")
        
        # Identifier les contraintes √† ajouter/modifier
        critical_issues = [
            issue for issue in quality_analysis["issues"] 
            if issue.severity == SeverityLevel.CRITICAL
        ]
        
        high_issues = [
            issue for issue in quality_analysis["issues"]
            if issue.severity == SeverityLevel.HIGH
        ]
        
        # Ajouter des contraintes plus strictes pour les probl√®mes critiques
        for issue in critical_issues:
            if issue.type == IssueType.TROU:
                # Renforcer les contraintes anti-trous
                self.optimizer.config["optimization_weights"]["no_gaps"] *= 1.5
                logger.info(f"Renforcement des contraintes anti-trous pour {issue.entity}")
        
        # Ajuster les poids pour les probl√®mes importants
        for issue in high_issues:
            if issue.type == IssueType.COURS_ISOLE:
                # Favoriser davantage les blocs
                self.optimizer.config["optimization_weights"]["block_courses"] *= 1.2
                logger.info("Augmentation du bonus pour les blocs de 2h")
    
    def _compile_final_result(self, best_solution: Dict, history: List[Dict], total_time: float) -> Dict:
        """Compile le r√©sultat final avec toutes les analyses"""
        if not best_solution:
            return {
                "success": False,
                "error": "Aucune solution viable trouv√©e",
                "iterations": len(history),
                "total_time": total_time
            }
        
        # Analyser les objectifs atteints
        objectives_status = self._check_objectives(best_solution)
        
        # Cr√©er le rapport final
        final_result = {
            "success": True,
            "final_score": best_solution.get("quality_analysis", {}).get("global_score", 0),
            "solution": best_solution,
            "objectives_met": sum(1 for obj in objectives_status.values() if obj["met"]),
            "objectives_status": objectives_status,
            "optimization_history": history,
            "performance": {
                "total_time_seconds": total_time,
                "iterations_run": len(history),
                "avg_time_per_iteration": total_time / len(history) if history else 0
            },
            "recommendations": self._generate_final_recommendations(best_solution),
            "export_ready": True
        }
        
        return final_result
    
    def _check_objectives(self, solution: Dict) -> Dict:
        """V√©rifie si les objectifs de qualit√© sont atteints"""
        stats = solution.get("stats", {})
        quality_analysis = solution.get("quality_analysis", {})
        
        objectives = {}
        
        # Objectif 1: Z√©ro trou
        objectives["zero_gaps"] = {
            "met": stats.get("total_gaps", 1) == 0,
            "current": stats.get("total_gaps", 0),
            "target": 0,
            "description": "Aucun trou dans les emplois du temps"
        }
        
        # Objectif 2: Pourcentage minimum de blocs de 2h
        total_hours = stats.get("blocks_2h", 0) + stats.get("isolated_hours", 0)
        block_percentage = (stats.get("blocks_2h", 0) / total_hours * 100) if total_hours > 0 else 0
        objectives["min_blocks_percentage"] = {
            "met": block_percentage >= self.config["target_metrics"]["min_blocks_percentage"],
            "current": round(block_percentage, 1),
            "target": self.config["target_metrics"]["min_blocks_percentage"],
            "description": "Minimum 80% des cours en blocs de 2h"
        }
        
        # Objectif 3: Amplitude maximale des professeurs
        avg_amplitude = stats.get("avg_teacher_amplitude", 0)
        objectives["max_teacher_amplitude"] = {
            "met": avg_amplitude <= self.config["target_metrics"]["max_teacher_amplitude"],
            "current": round(avg_amplitude, 1),
            "target": self.config["target_metrics"]["max_teacher_amplitude"],
            "description": "Amplitude moyenne ‚â§ 5h par jour"
        }
        
        # Objectif 4: Score de qualit√© global
        quality_score = quality_analysis.get("global_score", 0)
        objectives["quality_score_target"] = {
            "met": quality_score >= self.config["target_metrics"]["quality_score_target"],
            "current": quality_score,
            "target": self.config["target_metrics"]["quality_score_target"],
            "description": "Score global ‚â• 90/100"
        }
        
        return objectives
    
    def _generate_final_recommendations(self, solution: Dict) -> List[Dict]:
        """G√©n√®re des recommandations finales pour l'utilisateur"""
        recommendations = []
        
        quality_analysis = solution.get("quality_analysis", {})
        
        # Recommandations bas√©es sur les probl√®mes non r√©solus
        if quality_analysis.get("issues"):
            critical_count = len([i for i in quality_analysis["issues"] if i.severity == SeverityLevel.CRITICAL])
            high_count = len([i for i in quality_analysis["issues"] if i.severity == SeverityLevel.HIGH])
            
            if critical_count > 0:
                recommendations.append({
                    "type": "critical",
                    "title": "Probl√®mes critiques d√©tect√©s",
                    "description": f"{critical_count} probl√®me(s) critique(s) n√©cessitent une attention imm√©diate",
                    "action": "Voir les corrections automatiques sugg√©r√©es"
                })
            
            if high_count > 0:
                recommendations.append({
                    "type": "improvement",
                    "title": "Am√©liorations recommand√©es",
                    "description": f"{high_count} am√©lioration(s) peuvent augmenter la qualit√©",
                    "action": "Appliquer les suggestions de l'analyse d√©taill√©e"
                })
        
        # Recommandations g√©n√©rales
        stats = solution.get("stats", {})
        
        if stats.get("blocks_2h", 0) < stats.get("isolated_hours", 0):
            recommendations.append({
                "type": "optimization",
                "title": "Augmenter les blocs de 2h",
                "description": "Plus de cours pourraient √™tre regroup√©s en blocs cons√©cutifs",
                "action": "R√©viser les contraintes de disponibilit√© des professeurs"
            })
        
        if stats.get("avg_teacher_amplitude", 0) > 5:
            recommendations.append({
                "type": "workload",
                "title": "R√©duire l'amplitude des professeurs",
                "description": "Certains professeurs ont des journ√©es trop longues",
                "action": "Regrouper les cours sur moins de cr√©neaux"
            })
        
        return recommendations
    
    def export_schedule(self, solution: Dict, format: str = "json") -> Dict:
        """Exporte l'emploi du temps dans diff√©rents formats"""
        logger.info(f"Export de l'emploi du temps au format {format}")
        
        if format == "json":
            return {
                "export_format": "json",
                "timestamp": datetime.now().isoformat(),
                "schedule_data": solution,
                "summary": {
                    "quality_score": solution.get("final_score", 0),
                    "total_classes": len(solution.get("solution", {}).get("by_class", {})),
                    "total_teachers": len(solution.get("solution", {}).get("by_teacher", {})),
                    "total_courses": len(solution.get("solution", {}).get("schedule", []))
                }
            }
        
        # Autres formats peuvent √™tre ajout√©s ici (CSV, Excel, PDF, etc.)
        return {"error": f"Format {format} non support√©"}
    
    def run_full_analysis(self) -> Dict:
        """Lance une analyse compl√®te avec g√©n√©ration et export"""
        logger.info("=== ANALYSE COMPL√àTE DU SYST√àME ===")
        
        # 1. G√©n√©ration optimis√©e
        result = self.generate_optimal_schedule()
        
        if not result["success"]:
            return result
        
        # 2. Export automatique
        exported = self.export_schedule(result, "json")
        result["export"] = exported
        
        # 3. Rapport de synth√®se
        synthesis = {
            "execution_summary": {
                "total_time": result["performance"]["total_time_seconds"],
                "iterations": result["performance"]["iterations_run"],
                "final_quality": result["final_score"],
                "objectives_achieved": f"{result['objectives_met']}/{len(result['objectives_status'])}"
            },
            "key_metrics": {
                "zero_gaps_achieved": result["objectives_status"]["zero_gaps"]["met"],
                "block_percentage": result["objectives_status"]["min_blocks_percentage"]["current"],
                "avg_amplitude": result["objectives_status"]["max_teacher_amplitude"]["current"],
                "overall_score": result["final_score"]
            },
            "next_steps": result.get("recommendations", [])
        }
        
        result["synthesis"] = synthesis
        
        logger.info("=== ANALYSE COMPL√àTE TERMIN√âE ===")
        return result


# API FastAPI pour int√©gration
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import uvicorn

app = FastAPI(title="Advanced Schedule Optimizer API")

# Instance globale du syst√®me
scheduler_system = None

@app.on_event("startup")
async def startup_event():
    global scheduler_system
    scheduler_system = AdvancedSchedulingSystem()

@app.get("/")
async def root():
    return {"message": "Advanced Schedule Optimizer API", "version": "1.0.0"}

@app.post("/api/generate")
async def generate_schedule():
    """G√©n√®re un emploi du temps optimis√©"""
    try:
        result = scheduler_system.run_full_analysis()
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/api/config")
async def get_config():
    """Retourne la configuration actuelle"""
    return {
        "target_metrics": scheduler_system.config["target_metrics"],
        "optimization_settings": {
            "max_iterations": scheduler_system.config["max_iterations"],
            "quality_threshold": scheduler_system.config["quality_threshold"],
            "time_limit_per_iteration": scheduler_system.config["time_limit_per_iteration"]
        }
    }

if __name__ == "__main__":
    # Test direct
    system = AdvancedSchedulingSystem()
    result = system.run_full_analysis()
    
    print("\n=== R√âSULTAT FINAL ===")
    print(json.dumps({
        "success": result["success"],
        "final_score": result.get("final_score", 0),
        "objectives_met": f"{result.get('objectives_met', 0)}/{len(result.get('objectives_status', {}))}",
        "total_time": f"{result.get('performance', {}).get('total_time_seconds', 0):.1f}s"
    }, indent=2))
    
    # Lancer le serveur API si demand√©
    # uvicorn.run(app, host="0.0.0.0", port=8000)